# Snakemake pipeline for analyzing gwas summary statistic data using GFA
#
#
# LICENSE: CC0. Do what you want with the code, but it has no guarantees.
#          https://creativecommons.org/share-your-work/public-domain/cc0/
#
#
# source activate cause_large
#
# ./run_snakemake.sh
#
# don't forget to update cluster.yaml


import pandas as pd
import os

from snakemake.utils import validate

localrules: all, check_success, fail, status, final_file, cor_clust
###### Load configuration file
configfile: "config_gfa.yaml"
#validate(config, schema="schemas/config.schema.yaml")

# File organization
data_dir = config["out"]["data_dir"] #where the data is
out_dir = config["out"]["output_dir"] #where results will go
os.makedirs(out_dir, exist_ok = True)

prefix_dict = config["input"]

af_min = config["analysis"]["gfa"]["af_thresh"]
sstol_max = config["analysis"]["gfa"]["sample_size_tol"]
cond_num = "Inf" # conditioning happens in cor_clust step which is unique to gfa



include: "common_rules.smk"

R_strings = expand("{rt}_cc{cc}",
                   rt = R_strings,
                   cc = config["analysis"]["gfa"]["cor_clust"])

                   
## Set up GFA output
gfa_strings = expand("gfaseed{s}",
                     s = config["analysis"]["gfa"]["gfa_seed"])
                     
inp = expand(out_dir + "{prefix}_gls_loadings.{gfas}.ldpruned_{lds}.R_{rs}.Rgcor.RDS",
                prefix = prefix_dict.keys(),
                gfas = gfa_strings,
                lds = ld_strings,
                rs = R_strings)

rule all:
    input: inp

rule cor_clust:
    input: R = data_dir + "{prefix}_R_estimate.{rstring}.RDS"
    output: out = data_dir + "{prefix}_R_estimate.{rstring}_cc{cc}.RDS"
    params: cond_num = config["analysis"]["R"]["cond_num"]
    wildcard_constraints:
           cc = r"\d+(\.\d+)?"
    script: 'R/4_R_corr_clust.R'

# Run GFA
def R_input(wcs):
    global data_dir
    if wcs.Rstring.startswith("pt"):
        return f'{data_dir}{wcs.prefix}_R_estimate.ldpruned_{wcs.ldstring}.R_{wcs.Rstring}.RDS'
    else:
        return f'{data_dir}{wcs.prefix}_R_estimate.R_{wcs.Rstring}.RDS'


rule run_gfa:
    input: Z = expand(data_dir + "{{prefix}}_zmat.ldpruned_{{ldstring}}.{chrom}.RDS", chrom = range(1, 23)),
           R = R_input
    output:  out = out_dir + "{prefix}_gfa_gfaseed{fs}.ldpruned_{ldstring}.R_{Rstring}.1.RDS",
    params: params_file = config["analysis"]["gfa"]["gfa_params"],
            max_snps = config["analysis"]["gfa"]["max_snps"]
    wildcard_constraints: fs = r"\d+"
    script: 'R/5_run_gfa.R'

# This step refits if convergence was not reached
def refit_input(wcs):
    n = int(wcs.n)
    oldn = str(n-1)
    return f'{out_dir}{wcs.prefix}_gfa_gfaseed{wcs.fs}.ldpruned_{wcs.ldsctring}.R_{wcs.Rstring}.{oldn}.RDS'

rule refit_gfa:
    input:  inp = refit_input
    output: out = out_dir + "{prefix}_gfa_gfaseed{fs}.ldpruned_{ldstring}.R_{Rstring}.{n}.RDS",
    params: params_file = config["analysis"]["gfa"]["gfa_params"]
    script: 'R/5_refit_gfa.R'

# Below is snakemake machinery for checking convergence and re-running if necessary

max_gfa_tries =  int(config["analysis"]["gfa"]["maxrep"])

def next_input(wcs):
    global max_gfa_tries
    global out_dir

    check_prefix = f'{wcs.prefix}_check_gfaseed{wcs.fs}.ldpruned_{wcs.ldstring}.R_{wcs.Rstring}.'
    check_files = [y for y in os.listdir(out_dir) if y.startswith(check_prefix) ]
    n_tries = len(check_files) + 1

    success_file = f'{out_dir}{wcs.prefix}_success_gfaseed{wcs.fs}.ldpruned_{wcs.ldstring}.R_{wcs.Rstring}.txt'
    fail_file = f'{out_dir}{wcs.prefix}_fail_gfaseed{wcs.fs}.ldpruned_{wcs.ldstring}.R_{wcs.Rstring}.txt'
    if os.path.exists(success_file):
      print(success_file)
      return success_file
    elif n_tries > max_gfa_tries:
      print(fail_file)
      return fail_file
    else:
      print("back to checkpoint")
      checkpoints.check_success.get(n=n_tries, **wcs)

def final_rds(wcs):
    global out_dir

    check_prefix = f'{wcs.prefix}_check_{wcs.analysis}'
    check_files = [y for y in os.listdir(out_dir) if y.startswith(check_prefix) ]
    n_tries = len(check_files)
    final_gfa_file = f'{out_dir}{wcs.prefix}_gfa_{wcs.analysis}.{n_tries}.RDS'
    return final_gfa_file

checkpoint check_success:
    input: out_dir + "{prefix}_gfa_gfaseed{fs}.ldpruned_{ldstring}.R_{Rstring}.{n}.RDS"
    output: out_check = out_dir + "{prefix}_check_gfaseed{fs}.ldpruned_{ldstring}.R_{Rstring}.{n}.txt"
    params: success_file = out_dir + '{prefix}_success_gfaseed{fs}.ldpruned_{ldstring}.R_{Rstring}.txt'
    wildcard_constraints: n = r"\d+"
    script: "R/5_check_gfa.R"

rule fail:
    output: out_dir + '{prefix}_fail_gfaseed{fs}.ldpruned_{ldstring}.R_{Rstring}.txt'
    wildcard_constraints:  n = r"\d+"
    params: max_tries = max_gfa_tries
    shell: "echo  Model not converged after {params.max_tries} rounds of {maxiter} iterations. > {output} "

rule status:
    input:  next_input
    output: out = out_dir + "{prefix}_status_gfaseed{fs}.ldpruned_{ldstring}.R_{Rstring}.txt",
    shell: "cp {input} {output.out}"

rule final_file:
    input: status_file = out_dir + "{prefix}_status_{analysis}.txt",
    output: out_rds = out_dir + "{prefix}_gfa_{analysis}.final.RDS"
    params: rds = final_rds
    shell: "mv {params.rds} {output.out_rds}"

rule gls_loadings_chrom:
    input: z_file = data_dir + "{prefix}_zmat.{chrom}.RDS",
           gfa_file = out_dir + "{prefix}_gfa_gfaseed{fs}.ldpruned_{ldstring}.R_{Rstring}.final.RDS",
           R = R_input
    output: out = out_dir + "{prefix}_gls_loadings.gfaseed{fs}.ldpruned_{ldstring}.R_{Rstring}.{chrom}.RDS"
    script: "R/6_estL_gls.R"


rule R_ldsc_gls:
    input: Z = expand(out_dir + "{{prefix}}_gls_loadings.{{analysis}}.{chrom}.RDS", chrom = range(1, 23)),
           m = expand(l2_dir + "{chrom}.l2.M_5_50", chrom = range(1, 23)),
           l2 = expand(l2_dir + "{chrom}.l2.ldscore.gz", chrom = range(1, 23))
    output: out = out_dir + "{prefix}_gls_loadings.{analysis}.Rgcor.RDS"
    script: "R/6_estL_gencor.R"
